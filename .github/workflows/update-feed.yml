name: Update CSV feed from Google Sheets

on:
  schedule:
    - cron: "*/30 * * * *"
  workflow_dispatch: {}

# Required so GITHUB_TOKEN can push commits back to the repo
permissions:
  contents: write

jobs:
  update:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: true

      - name: Fetch CSV from Google Sheets
        run: |
          set -euxo pipefail
          mkdir -p data_new_updater

          curl -L --fail --show-error --silent \
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vT7xZnyhVukJaz32HSP1mvxl9PO_rZlUkXvcrsMIzDzrELpMM9M3UPpy_c9pGpaE5qTmO5aotvfh28l/pub?gid=852070582&single=true&output=csv" \
            -o data_new_updater/feed.csv

          echo "---- feed.csv preview ----"
          ls -lah data_new_updater/feed.csv
          head -n 3 data_new_updater/feed.csv

      - name: Convert feed.csv to JSONL + compact CSV + titles CSV + compact JSON
        run: |
          set -euxo pipefail
          python - <<'PY'
          import csv, json, pathlib, re
          from urllib.parse import urlparse

          in_path = pathlib.Path("data_new_updater/feed.csv")
          out_jsonl = pathlib.Path("data_new_updater/feed.jsonl")
          out_compact_csv = pathlib.Path("data_new_updater/feed_compact.csv")
          out_titles_csv = pathlib.Path("data_new_updater/feed_titles.csv")
          out_compact_json = pathlib.Path("data_new_updater/feed_compact.json")

          def norm_domain(u: str) -> str:
            try:
              d = urlparse(u).netloc.lower()
              if d.startswith("www."):
                d = d[4:]
              return d
            except Exception:
              return ""

          def clean_text(s) -> str:
            if s is None:
              return ""
            # collapse whitespace (incl. newlines/tabs) -> single spaces
            return re.sub(r"\s+", " ", str(s)).strip()

          def trunc(s, n=260) -> str:
            s = clean_text(s)
            return s[:n] + ("â€¦" if len(s) > n else "")

          rows = []
          with in_path.open("r", encoding="utf-8", newline="") as f:
            reader = csv.DictReader(f)
            for r in reader:
              url = (r.get("url") or "").strip()

              # Keep all original columns, but normalize key text fields used in analysis.
              out = dict(r)
              out["url"] = url
              out["title"] = clean_text(r.get("title") or "")
              out["snippet"] = clean_text(r.get("snippet") or "")
              out["published_date"] = clean_text(r.get("published_date") or "")
              out["source_domain"] = clean_text(r.get("source_domain") or "") or norm_domain(url)

              # Also normalize these if present (helps downstream tooling)
              if "feed_name_effective" in out:
                out["feed_name_effective"] = clean_text(out.get("feed_name_effective"))
              if "feed_name" in out:
                out["feed_name"] = clean_text(out.get("feed_name"))

              rows.append(out)

          # 1) JSONL: one JSON object per line
          with out_jsonl.open("w", encoding="utf-8", newline="\n") as f:
            for r in rows:
              f.write(json.dumps(r, ensure_ascii=False) + "\n")

          # 2) Compact CSV: sanitized fields so rows don't break on embedded newlines
          keys = []
          seen = set()
          for r in rows:
            for k in r.keys():
              if k not in seen:
                seen.add(k)
                keys.append(k)

          with out_compact_csv.open("w", encoding="utf-8", newline="") as f:
            w = csv.DictWriter(
              f,
              fieldnames=keys,
              extrasaction="ignore",
              lineterminator="\n"  # force LF so GitHub/raw viewers count lines correctly
            )
            w.writeheader()
            for r in rows:
              w.writerow(r)

          # 3) Titles-only CSV: small, analysis-friendly (title/url/query context)
          title_fields = ["published_date", "title", "url", "source_domain", "feed_name_effective", "feed_name"]
          with out_titles_csv.open("w", encoding="utf-8", newline="") as f:
            w = csv.DictWriter(
              f,
              fieldnames=title_fields,
              extrasaction="ignore",
              lineterminator="\n"  # force LF
            )
            w.writeheader()
            for r in rows:
              w.writerow({
                "published_date": clean_text(r.get("published_date","")),
                "title": trunc(r.get("title",""), 260),
                "url": (r.get("url","") or "").strip(),
                "source_domain": clean_text(r.get("source_domain","")),
                "feed_name_effective": clean_text(r.get("feed_name_effective","")),
                "feed_name": clean_text(r.get("feed_name","")),
              })

          # 4) Compact JSON (array): minified JSON for easy download/parse
          #    (Same content as rows, but minified to reduce size.)
          with out_compact_json.open("w", encoding="utf-8", newline="\n") as f:
            json.dump(rows, f, ensure_ascii=False, separators=(",", ":"))

          print("Wrote:", out_jsonl, out_compact_csv, out_titles_csv, out_compact_json)
          PY

          echo "---- Outputs: size + previews ----"
          ls -lah data_new_updater/feed.jsonl data_new_updater/feed_compact.csv data_new_updater/feed_titles.csv data_new_updater/feed_compact.json

          echo "---- Line counts ----"
          wc -l data_new_updater/feed.jsonl
          wc -l data_new_updater/feed_compact.csv
          wc -l data_new_updater/feed_titles.csv

          echo "---- CR/LF sanity (titles) ----"
          python - <<'PY'
          p="data_new_updater/feed_titles.csv"
          b=open(p,"rb").read()
          print("LF:", b.count(b"\n"), "CR:", b.count(b"\r"))
          PY

          echo "---- Head (titles) ----"
          head -n 5 data_new_updater/feed_titles.csv

      - name: Commit & push if changed
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euxo pipefail
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add -f \
            data_new_updater/feed.csv \
            data_new_updater/feed.jsonl \
            data_new_updater/feed_compact.csv \
            data_new_updater/feed_titles.csv \
            data_new_updater/feed_compact.json

          echo "---- Staged files ----"
          git diff --cached --name-only || true

          if git diff --cached --quiet; then
            echo "No staged changes."
            exit 0
          fi

          git commit -m "Update feed outputs"

          # Ensure token-authenticated push (avoids 'could not read Username' in Actions)
          git remote set-url origin "https://x-access-token:${GITHUB_TOKEN}@github.com/${GITHUB_REPOSITORY}.git"
          git push origin HEAD:main
