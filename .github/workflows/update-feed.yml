name: Update CSV feed from Google Sheets

on:
  schedule:
    - cron: "*/30 * * * *"
  workflow_dispatch: {}

# Needed so GITHUB_TOKEN can push commits back to the repo
permissions:
  contents: write

jobs:
  update:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: true

      - name: Fetch CSV from Google Sheets
        run: |
          set -euxo pipefail
          mkdir -p data_new_updater

          curl -L --fail --show-error --silent \
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vT7xZnyhVukJaz32HSP1mvxl9PO_rZlUkXvcrsMIzDzrELpMM9M3UPpy_c9pGpaE5qTmO5aotvfh28l/pub?gid=852070582&single=true&output=csv" \
            -o data_new_updater/feed.csv

          echo "---- File preview ----"
          ls -lah data_new_updater/feed.csv
          head -n 3 data_new_updater/feed.csv

      - name: Convert feed.csv to JSONL + compact CSV + titles-only CSV
        run: |
          set -euxo pipefail
          python - <<'PY'
          import csv, json, pathlib, re
          from urllib.parse import urlparse

          in_path = pathlib.Path("data_new_updater/feed.csv")
          out_jsonl = pathlib.Path("data_new_updater/feed.jsonl")
          out_compact = pathlib.Path("data_new_updater/feed_compact.csv")
          out_titles = pathlib.Path("data_new_updater/feed_titles.csv")

          def norm_domain(u: str) -> str:
            try:
              d = urlparse(u).netloc.lower()
              if d.startswith("www."):
                d = d[4:]
              return d
            except Exception:
              return ""

          def clean_text(s) -> str:
            if s is None:
              return ""
            return re.sub(r"\s+", " ", str(s)).strip()

          def trunc(s, n=260) -> str:
            s = clean_text(s)
            return s[:n] + ("â€¦" if len(s) > n else "")

          rows = []
          with in_path.open("r", encoding="utf-8", newline="") as f:
            reader = csv.DictReader(f)
            for r in reader:
              url = (r.get("url") or "").strip()
              out = dict(r)  # keep all original columns

              out["url"] = url
              out["title"] = clean_text(r.get("title") or "")
              out["snippet"] = clean_text(r.get("snippet") or "")
              out["published_date"] = clean_text(r.get("published_date") or "")
              out["source_domain"] = clean_text(r.get("source_domain") or "") or norm_domain(url)

              rows.append(out)

          # JSONL: one JSON object per line
          with out_jsonl.open("w", encoding="utf-8") as f:
            for r in rows:
              f.write(json.dumps(r, ensure_ascii=False) + "\n")

          # Compact CSV: sanitized fields so rows don't "break" on embedded newlines
          keys = []
          seen = set()
          for r in rows:
            for k in r.keys():
              if k not in seen:
                seen.add(k)
                keys.append(k)

          with out_compact.open("w", encoding="utf-8", newline="") as f:
            w = csv.DictWriter(f, fieldnames=keys, extrasaction="ignore")
            w.writeheader()
            for r in rows:
              w.writerow(r)

          # Titles-only CSV: small, analysis-friendly (title/url/query context)
          title_fields = ["published_date", "title", "url", "source_domain", "feed_name_effective", "feed_name"]
          with out_titles.open("w", encoding="utf-8", newline="") as f:
            w = csv.DictWriter(f, fieldnames=title_fields, extrasaction="ignore")
            w.writeheader()
            for r in rows:
              w.writerow({
                "published_date": clean_text(r.get("published_date","")),
                "title": trunc(r.get("title",""), 260),
                "url": (r.get("url","") or "").strip(),
                "source_domain": clean_text(r.get("source_domain","")),
                "feed_name_effective": clean_text(r.get("feed_name_effective","")),
                "feed_name": clean_text(r.get("feed_name","")),
              })

          print("Wrote:", out_jsonl, out_compact, out_titles)
          PY

          echo "---- Derived files preview ----"
          ls -lah data_new_updater/feed.jsonl data_new_updater/feed_compact.csv data_new_updater/feed_titles.csv
          head -n 2 data_new_updater/feed.jsonl
          head -n 3 data_new_updater/feed_compact.csv
          head -n 3 data_new_updater/feed_titles.csv

      - name: Commit & push if changed
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euxo pipefail
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add -f \
            data_new_updater/feed.csv \
            data_new_updater/feed.jsonl \
            data_new_updater/feed_compact.csv \
            data_new_updater/feed_titles.csv

          echo "---- Staged files ----"
          git diff --cached --name-only || true

          if git diff --cached --quiet; then
            echo "No staged changes."
            exit 0
          fi

          git commit -m "Update feed outputs"

          # Ensure push uses token auth (fixes 'could not read Username' in Actions)
          git remote set-url origin "https://x-access-token:${GITHUB_TOKEN}@github.com/${GITHUB_REPOSITORY}.git"
          git push origin HEAD:main
